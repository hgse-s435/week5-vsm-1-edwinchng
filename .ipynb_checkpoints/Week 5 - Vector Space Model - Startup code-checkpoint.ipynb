{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 - Vector Space Model (VSM) and Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the next weeks, we are going to re-implement Sherin's algorithm and apply it to the text data we've been working on last week! Here's our roadmap:\n",
    "\n",
    "**Week 5 - data cleaning**\n",
    "1. import the data\n",
    "2. clean the data (e.g., remopve stop words, punctuation, etc.)\n",
    "3. build a vocabulary for the dataset\n",
    "4. create chunks of 100 words, with a 25-words overlap\n",
    "5. create a word count matrix, where each chunk of a row and each column represents a word\n",
    "\n",
    "**Week 6 - vectorization and linear algebra**\n",
    "6. Dampen: weight the frequency of words (1 + log[count])\n",
    "7. Scale: Normalize weighted frequency of words\n",
    "8. Direction: compute deviation vectors\n",
    "\n",
    "**Week 7 - Clustering**\n",
    "9. apply different unsupervised machine learning algorithms\n",
    "    * figure out how many clusters we want to keep\n",
    "    * inspect the results of the clustering algorithm\n",
    "\n",
    "**Week 8 - Visualizing the results**\n",
    "10. create visualizations to compare documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in python code, our goal is to recreate the steps above as functions\n",
    "# so that we can just one line to run topic modeling on a list of \n",
    "# documents: \n",
    "def ExtractTopicsVSM(documents, numTopics):\n",
    "    ''' this functions takes in a list of documents (strings), \n",
    "        runs topic modeling (as implemented by Sherin, 2013)\n",
    "        and returns the clustering results, the matrix used \n",
    "        for clustering a visualization '''\n",
    "    \n",
    "    # step 2: clean up the documents\n",
    "    documents = clean_list_of_documents(documents)\n",
    "    \n",
    "    # step 3: let's build the vocabulary of these docs\n",
    "    vocabulary = get_vocabulary(documents)\n",
    "    \n",
    "    # step 4: we build our list of 100-words overlapping fragments\n",
    "    documents = flatten_and_overlap(documents)\n",
    "    \n",
    "    # step 5: we convert the chunks into a matrix\n",
    "    matrix = docs_by_words_matrix(documents, vocabulary)\n",
    "    \n",
    "    # step 6: we weight the frequency of words (count = 1 + log(count))\n",
    "    matrix = one_plus_log_mat(matrix, documents, vocabulary)\n",
    "    \n",
    "    # step 7: we normalize the matrix\n",
    "    matrix = normalize(matrix)\n",
    "    \n",
    "    # step 8: we compute deviation vectors\n",
    "    matrix = transform_deviation_vectors(matrix, documents)\n",
    "    \n",
    "    # step 9: we apply a clustering algorithm to find topics\n",
    "    results_clustering = cluster_matrix(matrix)\n",
    "    \n",
    "    # step 10: we create a visualization of the topics\n",
    "    visualization = visualize_clusters(results_clustering, vocabulary)\n",
    "    \n",
    "    # finally, we return the clustering results, the matrix, and a visualization\n",
    "    return results_clustering, matrix, visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) using glob, find all the text files in the \"Papers\" folder\n",
    "# Hint: refer to last week's notebook\n",
    "import glob\n",
    "files = glob.glob('./Papers/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) get all the data from the text files into the \"documents\" list\n",
    "# P.S. make sure you use the 'utf-8' encoding\n",
    "documents = []\n",
    "\n",
    "for file in files:\n",
    "    with open(file,'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        documents.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n",
      "\n",
      "\f",
      "epistemic network analysis and topic modeling for chat\n",
      "data from collaborative learning environment\n",
      "zhiqiang cai\n",
      "\n",
      "brendan eagan\n",
      "\n",
      "nia m. dowell\n",
      "\n",
      "the university of memphis\n",
      "365 innovation drive, suite 410\n",
      "memphis, tn, usa\n",
      "\n",
      "university of wisconsin-madison\n",
      "1025 west johnson street\n",
      "madison, wi, usa\n",
      "\n",
      "the university of memphis\n",
      "365 innovation drive, suite 410\n",
      "memphis, tn, usa\n",
      "\n",
      "zcai@memphis.edu\n",
      "\n",
      "eaganb@gmail.com\n",
      "\n",
      "niadowell@gmail.com\n",
      "\n",
      "james w. pennebaker\n",
      "\n",
      "david w. shaffer\n",
      "\n",
      "arthur c. graesser\n",
      "\n",
      "university of texas-austin\n",
      "116 inner campus dr stop g6000\n",
      "austin, tx, usa\n",
      "\n",
      "university of wisconsin-madison\n",
      "1025 west johnson street\n",
      "madison, wi, usa\n",
      "\n",
      "the university of memphis\n",
      "365 innovation drive, suite 403\n",
      "memphis, tn, usa\n",
      "\n",
      "pennebaker@utexas.edu\n",
      "\n",
      "dws@education.wisc.edu\n",
      "\n",
      "art.graesser@gmail.com\n",
      "\n",
      "abstract\n",
      "this study investigates a possible way to analyze chat data from\n",
      "collaborative learning environments using epistemic network\n",
      "analysis and topic modeling. a 300-topic general topic model\n",
      "built from tasa\n"
     ]
    }
   ],
   "source": [
    "# 3) print the first 1000 characters of the first document to see what it \n",
    "# looks like (we'll use this as a sanity check below)\n",
    "print(documents[0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 4) only select the text that's between the first occurence of the \n",
    "# the word \"abstract\" and the last occurence of the word \"reference\"\n",
    "# Optional: print the length of the string before and after, as a \n",
    "# sanity check\n",
    "# HINT: https://stackoverflow.com/questions/14496006/finding-last-occurrence-of-substring-in-string-replacing-that\n",
    "# read more about rfind: https://www.tutorialspoint.com/python/string_rfind.htm\n",
    "\n",
    "contents = []\n",
    "\n",
    "for doc in documents:\n",
    "    start = doc.index('abstract') + len('abstract')\n",
    "    end = doc.rfind('reference')\n",
    "    contents.append(doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this study investigates a possible way to analyze chat data from collaborative learning environments using epistemic network analysis and topic modeling. a 300-topic general topic model built from tasa (touchstone applied science associates) corpus was used in this study. 300 topic scores for each of the 15,670 utterances in our chat data were computed. seven relevant topics were selected based on the total document scores. while the aggregated topic scores had some power in predicting students‚Äô learning, using epistemic network analysis enables assessing the data from a different angle. the results showed that the topic score based epistemic networks between low gain students and high gain students were significantly different (ùë° = 2.00). overall, the results suggest these two analytical approaches provide complementary information and afford new insights into the processes related to successful collaborative interactions.  keywords chat; collaborative learning; topic modeling; epist\n"
     ]
    }
   ],
   "source": [
    "# 5) replace carriage returns (i.e., \"\\n\") with a white space\n",
    "# check that the result looks okay by printing the \n",
    "# first 1000 characters of the 1st doc:\n",
    "\n",
    "for i in range(0,len(contents)):\n",
    "    contents[i] = contents[i].replace('\\n',' ')\n",
    "\n",
    "print(contents[0][:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this study investigates a possible way to analyze chat data from collaborative learning environments using epistemic network analysis and topic modeling  a 300 topic general topic model built from tasa  touchstone applied science associates  corpus was used in this study  300 topic scores for each of the 15 670 utterances in our chat data were computed  seven relevant topics were selected based on the total document scores  while the aggregated topic scores had some power in predicting students  learning  using epistemic network analysis enables assessing the data from a different angle  the results showed that the topic score based epistemic networks between low gain students and high gain students were significantly different  ùë°   2 00   overall  the results suggest these two analytical approaches provide complementary information and afford new insights into the processes related to successful collaborative interactions   keywords chat  collaborative learning  topic modeling  epist\n"
     ]
    }
   ],
   "source": [
    "# 6) replace the punctation below by a white space\n",
    "# check that the result looks okay \n",
    "# (e.g., by print the first 1000 characters of the 1st doc)\n",
    "\n",
    "punctuation = ['.', '...', '!', '#', '\"', '%', '$', \"'\", '&', ')', \n",
    "               '(', '+', '*', '-', ',', '/', '.', ';', ':', '=', \n",
    "               '<', '?', '>', '@', '\",', '\".', '[', ']', '\\\\', ',',\n",
    "               '_', '^', '`', '{', '}', '|', '~', '‚àí', '‚Äù', '‚Äú', '‚Äô']\n",
    "\n",
    "for i in range(0,len(contents)):\n",
    "    for p in punctuation:\n",
    "        contents[i] = contents[i].replace(p,' ')\n",
    "\n",
    "print(contents[0][:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this study investigates a possible way to analyze chat data from collaborative learning environments using epistemic network analysis and topic modeling  a     topic general topic model built from tasa  touchstone applied science associates  corpus was used in this study      topic scores for each of the        utterances in our chat data were computed  seven relevant topics were selected based on the total document scores  while the aggregated topic scores had some power in predicting students  learning  using epistemic network analysis enables assessing the data from a different angle  the results showed that the topic score based epistemic networks between low gain students and high gain students were significantly different  ùë°          overall  the results suggest these two analytical approaches provide complementary information and afford new insights into the processes related to successful collaborative interactions   keywords chat  collaborative learning  topic modeling  epist\n"
     ]
    }
   ],
   "source": [
    "# 7) remove numbers by either a white space or the word \"number\"\n",
    "# again, print the first 1000 characters of the first document\n",
    "# to check that you're doing the right thing\n",
    "\n",
    "numbers = ['1', '2', '3', '4', '5', '6','7', '8', '9', '0' ]\n",
    "\n",
    "for i in range(0,len(contents)):\n",
    "    for n in numbers:\n",
    "        contents[i] = contents[i].replace(n,' ')\n",
    "\n",
    "print(contents[0][:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " study investigates possible way analyze chat data collaborative learning environments using epistemic network analysis topic modeling      topic general topic model built tasa  touchstone applied science associates  corpus used study      topic scores        utterances chat data computed  seven relevant topics selected based total document scores  aggregated topic scores power predicting students  learning  using epistemic network analysis enables assessing data different angle  results showed topic score based epistemic networks low gain students high gain students significantly different  ùë°          overall  results suggest two analytical approaches provide complementary information afford new insights processes related successful collaborative interactions   keywords chat  collaborative learning  topic modeling  epistemic network analysis     introduction collaborative learning special form learning interaction affords opportunities groups students combine cognitive resources synch\n"
     ]
    }
   ],
   "source": [
    "# 8) Remove the stop words below from our documents\n",
    "# print the first 1000 characters of the first document\n",
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', \n",
    "              'ourselves', 'you', 'your', 'yours', 'yourself', \n",
    "              'yourselves', 'he', 'him', 'his', 'himself', 'she', \n",
    "              'her', 'hers', 'herself', 'it', 'its', 'itself', \n",
    "              'they', 'them', 'their', 'theirs', 'themselves', \n",
    "              'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "              'these', 'those', 'am', 'is', 'are', 'was', 'were', \n",
    "              'be', 'been', 'being', 'have', 'has', 'had', 'having', \n",
    "              'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', \n",
    "              'but', 'if', 'or', 'because', 'as', 'until', 'while', \n",
    "              'of', 'at', 'by', 'for', 'with', 'about', 'against', \n",
    "              'between', 'into', 'through', 'during', 'before', \n",
    "              'after', 'above', 'below', 'to', 'from', 'up', 'down', \n",
    "              'in', 'out', 'on', 'off', 'over', 'under', 'again', \n",
    "              'further', 'then', 'once', 'here', 'there', 'when', \n",
    "              'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
    "              'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
    "              'nor', 'not', 'only', 'own', 'same', 'so', 'than', \n",
    "              'too', 'very', 's', 't', 'can', 'will', \n",
    "              'just', 'don', 'should', 'now']\n",
    "\n",
    "for i in range(0,len(stop_words)):\n",
    "    stop_words[i] = ' ' + stop_words[i] + ' '\n",
    "\n",
    "for i in range(0,len(contents)):\n",
    "    for s in stop_words:\n",
    "        contents[i] = contents[i].replace(s,' ')\n",
    "\n",
    "print(contents[0][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " study investigates possible way analyze chat data collaborative learning environments using epistemic network analysis topic modeling      topic general topic model built tasa  touchstone applied science associates  corpus used study      topic scores        utterances chat data computed  seven relevant topics selected based total document scores  aggregated topic scores power predicting students  learning  using epistemic network analysis enables assessing data different angle  results showed topic score based epistemic networks low gain students high gain students significantly different          overall  results suggest two analytical approaches provide complementary information afford new insights processes related successful collaborative interactions   keywords chat  collaborative learning  topic modeling  epistemic network analysis     introduction collaborative learning special form learning interaction affords opportunities groups students combine cognitive resources synchron\n"
     ]
    }
   ],
   "source": [
    "# 9) remove words with one and two characters (e.g., 'd', 'er', etc.)\n",
    "# print the first 1000 characters of the first document\n",
    "\n",
    "import re\n",
    "\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "\n",
    "for i in range(0,len(contents)):\n",
    "    contents[i] = shortword.sub('', contents[i])\n",
    "\n",
    "print(contents[0][:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) package all of your work above into a function that cleans a given document\n",
    "\n",
    "def clean_list_of_documents(documents):\n",
    "    \n",
    "    ### your code ###\n",
    "    \n",
    "    import re\n",
    "    \n",
    "    cleaned_docs = []\n",
    "    remove = ['\\n',\n",
    "              '.', '...', '!', '#', '\"', '%', '$', \"'\", '&', ')','(', '+', '*', '-', ',', '/', '.', ';', ':', '=', \n",
    "              '<', '?', '>', '@', '\",', '\".', '[', ']', '\\\\', ',', '_', '^', '`', '{', '}', '|', '~', '‚àí', '‚Äù', '‚Äú', '‚Äô',\n",
    "              '1', '2', '3', '4', '5', '6','7', '8', '9', '0',\n",
    "              ' i ', ' me ', ' my ', ' myself ', ' we ', ' our ', ' ours ', \n",
    "              ' ourselves ', ' you ', ' your ', ' yours ', ' yourself ', \n",
    "              ' yourselves ', ' he ', ' him ', ' his ', ' himself ', ' she ', \n",
    "              ' her ', ' hers ', ' herself ', ' it ', ' its ', ' itself ', \n",
    "              ' they ', ' them ', ' their ', ' theirs ', ' themselves ', \n",
    "              ' what ', ' which ', ' who ', ' whom ', ' this ', ' that ', \n",
    "              ' these ', ' those ', ' am ', ' is ', ' are ', ' was ', ' were ', \n",
    "              ' be ', ' been ', ' being ', ' have ', ' has ', ' had ', ' having ', \n",
    "              ' do ', ' does ', ' did ', ' doing ', ' a ', ' an ', ' the ', ' and ', \n",
    "              ' but ', ' if ', ' or ', ' because ', ' as ', ' until ', ' while ', \n",
    "              ' of ', ' at ', ' by ', ' for ', ' with ', ' about ', ' against ', \n",
    "              ' between ', ' into ', ' through ', ' during ', ' before ', \n",
    "              ' after ', ' above ', ' below ', ' to ', ' from ', ' up ', ' down ', \n",
    "              ' in ', ' out ', ' on ', ' off ', ' over ', ' under ', ' again ', \n",
    "              ' further ', ' then ', ' once ', ' here ', ' there ', ' when ', \n",
    "              ' where ', ' why ', ' how ', ' all ', ' any ', ' both ', ' each ', \n",
    "              ' few ', ' more ', ' most ', ' other ', ' some ', ' such ', ' no ', \n",
    "              ' nor ', ' not ', ' only ', ' own ', ' same ', ' so ', ' than ', \n",
    "              ' too ', ' very ', ' s ', ' t ', ' can ', ' will ', \n",
    "              ' just ', ' don ', ' should ', ' now ']\n",
    "    shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "    \n",
    "    for doc in documents:\n",
    "        start = doc.index('abstract') + len('abstract')\n",
    "        end = doc.rfind('reference')\n",
    "        cleaned_docs.append(doc[start:end])\n",
    "    \n",
    "    for i in range(0,len(cleaned_docs)):\n",
    "        for r in remove:\n",
    "            cleaned_docs[i] = cleaned_docs[i].replace(r,' ')\n",
    "    \n",
    "        cleaned_docs[i] = shortword.sub('', cleaned_docs[i])\n",
    "    \n",
    "    return cleaned_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " study investigates possible way analyze chat data collaborative learning environments using epistemic network analysis topic modeling      topic general topic model built tasa  touchstone applied science associates  corpus used study      topic scores        utterances chat data computed  seven relevant topics selected based total document scores  aggregated topic scores power predicting students  learning  using epistemic network analysis enables assessing data different angle  results showed topic score based epistemic networks low gain students high gain students significantly different          overall  results suggest two analytical approaches provide complementary information afford new insights processes related successful collaborative interactions   keywords chat  collaborative learning  topic modeling  epistemic network analysis     introduction collaborative learning special form learning interaction affords opportunities groups students combine cognitive resources synchron\n"
     ]
    }
   ],
   "source": [
    "# 11a) reimport your raw data using the code in 2)\n",
    "documents = []\n",
    "\n",
    "for file in files:\n",
    "    with open(file,'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        documents.append(text)\n",
    "        \n",
    "# 11b) clean your files using the function above\n",
    "\n",
    "cleaned_documents = clean_list_of_documents(documents)\n",
    "\n",
    "# 11c) print the first 1000 characters of the first document\n",
    "\n",
    "print(cleaned_documents[0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Build your list of vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list of words (i.e., the vocabulary) is going to become the columns of your matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12) Describe why we need to figure out the vocabulary used in our corpus (refer back to Sherin's paper, and explain in your own words): \n",
    "\n",
    "We require the vocabulary as a reference for us to be able to convert any given text into a vector (i.e the vocabulary forms the basis for our vector space.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5660\n"
     ]
    }
   ],
   "source": [
    "# 13) create a function that takes in a list of documents\n",
    "# and returns a set of unique words. Make sure that you\n",
    "# sort the list alphabetically before returning it. \n",
    "\n",
    "def get_vocabulary(documents):\n",
    "    voc = []\n",
    "    \n",
    "    ### your code ###\n",
    "    for i in range(0,len(documents)):\n",
    "        words = documents[i].split()\n",
    "        for word in words:\n",
    "            if word in voc:\n",
    "                pass\n",
    "            else:\n",
    "                voc.append(word)\n",
    "    \n",
    "    return voc\n",
    "\n",
    "# Then print the length of your vocabulary (it should be \n",
    "# around 5500 words)\n",
    "vocabulary = get_vocabulary(cleaned_documents)\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sherin's vocabulary consists of 647 words.\n"
     ]
    }
   ],
   "source": [
    "# 14) what was the size of Sherin's vocabulary? \n",
    "print('Sherin\\'s vocabulary consists of 647 words.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - transform your documents into 100-words chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 15) create a function that takes in a list of documents\n",
    "# and returns a list of 100-words chunk \n",
    "# (with a 25 words overlap between them)\n",
    "# Optional: add two arguments, one for the number of words\n",
    "# in each chunk, and one for the overlap size\n",
    "# Advice: combining all the documents into one giant string\n",
    "# and splitting it into separate words will make your life easier!\n",
    "\n",
    "def chunks(documents, chunk_size, overlap_size):\n",
    "    list_of_chunk = []\n",
    "    giant_string = ''\n",
    "    short_string = ''\n",
    "    \n",
    "    for doc in documents:\n",
    "        giant_string = giant_string + doc\n",
    "    \n",
    "    list_of_words = giant_string.split()\n",
    "    \n",
    "    for i in range(0,len(list_of_words),overlap_size):\n",
    "        if i+100 < len(list_of_words):\n",
    "            for x in range(0,chunk_size):\n",
    "                y = i + x\n",
    "                short_string = short_string + list_of_words[y] + ' '\n",
    "            \n",
    "            list_of_chunk.append(short_string)\n",
    "            short_string=''\n",
    "            \n",
    "    return list_of_chunk\n",
    "\n",
    "word_chunks = chunks(cleaned_documents,100,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16) create a for loop to double check that each chunk has \n",
    "# a length of 100\n",
    "# Optional: use assert to do this check\n",
    "\n",
    "for chunk in word_chunks:\n",
    "    assert len(chunk.split()) == 100\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---first chunk---\n",
      "study investigates possible way analyze chat data collaborative learning environments using epistemic network analysis topic modeling topic general topic model built tasa touchstone applied science associates corpus used study topic scores utterances chat data computed seven relevant topics selected based total document scores aggregated topic scores power predicting students learning using epistemic network analysis enables assessing data different angle results showed topic score based epistemic networks low gain students high gain students significantly different overall results suggest two analytical approaches provide complementary information afford new insights processes related successful collaborative interactions keywords chat collaborative learning topic modeling epistemic network analysis \n",
      "\n",
      "---original---\n",
      "study investigates possible way analyze chat data collaborative learning environments using epistemic network analysis topic modeling topic general topic model built tasa touchstone applied science associates corpus used study topic scores utterances chat data computed seven relevant topics selected based total document scores aggregated topic scores power predicting students learning using epistemic network analysis enables assessing data different angle results showed topic score based epistemic networks low gain students high gain students significantly different overall results suggest two analytical approaches provide complementary information afford new insights processes related successful collaborative interactions keywords chat collaborative learning topic modeling epistemic network analysis \n"
     ]
    }
   ],
   "source": [
    "# 17) print the first chunk, and compare it to the original text.\n",
    "# does that match what Sherin describes in his paper?\n",
    "\n",
    "original = ''\n",
    "\n",
    "for i in range(0,100):\n",
    "    original = original + cleaned_documents[0].split()[i] + ' '\n",
    "\n",
    "print('---first chunk---\\n'+ word_chunks[0] + '\\n')\n",
    "print('---original---\\n'+ original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sherin had 794 segments of text.\n",
      "A chunk would be converted to a vector in the next step of topic modeling algorithm.\n"
     ]
    }
   ],
   "source": [
    "# 18) how many chunks did Sherin have? What does a chunk become \n",
    "# in the next step of our topic modeling algorithm? \n",
    "print('Sherin had 794 segments of text.')\n",
    "print('A chunk would be converted to a vector in the next step of topic modeling algorithm.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) We could reduce our vocabulary by selecting high frequency words or meaningful words based on purpose of study.\n",
      "2) We could reduce all the words to its infinitive form so that only words with the same or similar meaning would be captured.\n"
     ]
    }
   ],
   "source": [
    "# 19) what are some other preprocessing steps we could do \n",
    "# to improve the quality of the text data? Mention at least 2.\n",
    "print('1) We could reduce our vocabulary by selecting high frequency words or meaningful words based on purpose of study.')\n",
    "print('2) We could reduce all the words to its infinitive form so that only words with the same or similar meaning would be captured.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5 - This step creates commonly used vector and matrix operations such as dot product in order for us to be able to manipulate and analyze our text after its conversion into vectors.\n",
      "Step 6 - This step modifies the raw frequency counts by using the function (1+log(count)) so that the count for high frequency words is brought down.\n",
      "Step 7 - This step changes the length of all the vectors to be of 1 unit length so the vectors could be standardized for analysis.\n",
      "Step 8 - This step calculates the deviation vector for each vector, which is a measure of how dissimilar a given text is from the average text.\n",
      "Step 9 - This step aims to group the vectors or texts that are similar in meaning together.\n",
      "Step 10 - This step creates visualization tools such as bar graphs for the human analyst to attain better overview and comprehension.\n"
     ]
    }
   ],
   "source": [
    "# 20) in your own words, describe the next steps of the \n",
    "# data modeling algorithms (listed below):\n",
    "print('Step 5 - This step creates commonly used vector and matrix operations such as dot product in order for us to be able to manipulate and analyze our text after its conversion into vectors.')\n",
    "print('Step 6 - This step modifies the raw frequency counts by using the function (1+log(count)) so that the count for high frequency words is brought down.')\n",
    "print('Step 7 - This step changes the length of all the vectors to be of 1 unit length so the vectors could be standardized for analysis.')\n",
    "print('Step 8 - This step calculates the deviation vector for each vector, which is a measure of how dissimilar a given text is from the average text.')\n",
    "print('Step 9 - This step aims to group the vectors or texts that are similar in meaning together.')\n",
    "print('Step 10 - This step creates visualization tools such as bar graphs for the human analyst to attain better overview and comprehension.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Vector and Matrix operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Weight word frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Matrix normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 - Deviation Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10 - Visualizing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Step - Putting it all together: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in python code, our goal is to recreate the steps above as functions\n",
    "# so that we can just one line to run topic modeling on a list of \n",
    "# documents: \n",
    "def ExtractTopicsVSM(documents, numTopics):\n",
    "    ''' this functions takes in a list of documents (strings), \n",
    "        runs topic modeling (as implemented by Sherin, 2013)\n",
    "        and returns the clustering results, the matrix used \n",
    "        for clustering a visualization '''\n",
    "    \n",
    "    # step 2: clean up the documents\n",
    "    documents = clean_list_of_documents(documents)\n",
    "    \n",
    "    # step 3: let's build the vocabulary of these docs\n",
    "    vocabulary = get_vocabulary(documents)\n",
    "    \n",
    "    # step 4: we build our list of 100-words overlapping fragments\n",
    "    documents = flatten_and_overlap(documents)\n",
    "    \n",
    "    # step 5: we convert the chunks into a matrix\n",
    "    matrix = docs_by_words_matrix(documents, vocabulary)\n",
    "    \n",
    "    # step 6: we weight the frequency of words (count = 1 + log(count))\n",
    "    matrix = one_plus_log_mat(matrix, documents, vocabulary)\n",
    "    \n",
    "    # step 7: we normalize the matrix\n",
    "    matrix = normalize(matrix)\n",
    "    \n",
    "    # step 8: we compute deviation vectors\n",
    "    matrix = transform_deviation_vectors(matrix, documents)\n",
    "    \n",
    "    # step 9: we apply a clustering algorithm to find topics\n",
    "    results_clustering = cluster_matrix(matrix)\n",
    "    \n",
    "    # step 10: we create a visualization of the topics\n",
    "    visualization = visualize_clusters(results_clustering, vocabulary)\n",
    "    \n",
    "    # finally, we return the clustering results, the matrix, and a visualization\n",
    "    return results_clustering, matrix, visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
